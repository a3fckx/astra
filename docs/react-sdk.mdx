---
title: React SDK
subtitle: 'Agents Platform SDK: deploy customized, interactive voice agents in minutes.'
---

<Info>
  Refer to the [Agents Platform overview](/docs/agents-platform/overview) for an explanation of how
  Agents Platform works.
</Info>

<iframe
  width="100%"
  height="400"
  src="https://www.youtube-nocookie.com/embed/ftf-8F91bAc?rel=0&autoplay=0"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  allowfullscreen
></iframe>

## Installation

Install the package in your project through package manager.

```shell
npm install @elevenlabs/react
# or
yarn add @elevenlabs/react
# or
pnpm install @elevenlabs/react
```

## Usage

### useConversation

A React hook for managing connection and audio usage for ElevenLabs Agents.

#### Initialize conversation

First, initialize the Conversation instance.

```tsx
import { useConversation } from '@elevenlabs/react';

const conversation = useConversation();
```

Note that Agents Platform requires microphone access. Consider explaining and allowing access in your app's UI before the Conversation starts.

```js
// call after explaining to the user why the microphone access is needed
await navigator.mediaDevices.getUserMedia({ audio: true });
```

#### Options

The Conversation can be optionally initialized with certain parameters.

```tsx
const conversation = useConversation({
  /* options object */
});
```

Options include:

- **clientTools** - object definition for client tools that can be invoked by agent. [See below](#client-tools) for details.
- **overrides** - object definition conversations settings overrides. [See below](#conversation-overrides) for details.
- **textOnly** - whether the conversation should run in text-only mode. [See below](#text-only) for details.
- **serverLocation** - specify the server location (`"us"`, `"eu-residency"`, `"in-residency"`, `"global"`). Defaults to `"us"`.

#### Callbacks Overview

- **onConnect** - handler called when the conversation websocket connection is established.
- **onDisconnect** - handler called when the conversation websocket connection is ended.
- **onMessage** - handler called when a new message is received. These can be tentative or final transcriptions of user voice, replies produced by LLM, or debug message when a debug option is enabled.
- **onError** - handler called when a error is encountered.
- **onAudio** - handler called when audio data is received.
- **onModeChange** - handler called when the conversation mode changes (speaking/listening).
- **onStatusChange** - handler called when the connection status changes.
- **onCanSendFeedbackChange** - handler called when the ability to send feedback changes.
- **onDebug** - handler called when debug information is available.
- **onUnhandledClientToolCall** - handler called when an unhandled client tool call is encountered.
- **onVadScore** - handler called when voice activity detection score changes.

##### Latency tracking

The SDK does not expose a dedicated latency API, but you can measure perceived turn latency by timing events that already flow through the callbacks you control. A common pattern is to timestamp the latest user transcript and resolve the timer when the agent starts speaking:

```tsx
const latencyStartRef = useRef<number | null>(null);
const [lastLatencyMs, setLastLatencyMs] = useState<number | null>(null);

const conversation = useConversation({
  onMessage: (payload) => {
    if (!payload) return;

    if (payload.source === 'user') {
      latencyStartRef.current = performance.now();
    }

    if (payload.source === 'ai' && latencyStartRef.current) {
      setLastLatencyMs(Math.round(performance.now() - latencyStartRef.current));
      latencyStartRef.current = null;
    }
  },
  onAudio: () => {
    if (latencyStartRef.current) {
      setLastLatencyMs(Math.round(performance.now() - latencyStartRef.current));
      latencyStartRef.current = null;
    }
  },
});
```

For deeper diagnostics you can also forward any `onDebug` payloads to your own logger. When the agent is configured to emit timing metadata, those events surface there without blocking voice flow.

##### Client Tools

Client tools are a way to enable agent to invoke client-side functionality. This can be used to trigger actions in the client, such as opening a modal or doing an API call on behalf of the user.

Client tools definition is an object of functions, and needs to be identical with your configuration within the [ElevenLabs UI](https://elevenlabs.io/app/agents), where you can name and describe different tools, as well as set up the parameters passed by the agent.

```ts
const conversation = useConversation({
  clientTools: {
    displayMessage: (parameters: { text: string }) => {
      alert(text);

      return 'Message displayed';
    },
  },
});
```

In case function returns a value, it will be passed back to the agent as a response.

Note that the tool needs to be explicitly set to be blocking conversation in ElevenLabs UI for the agent to await and react to the response, otherwise agent assumes success and continues the conversation.

##### Conversation overrides

You may choose to override various settings of the conversation and set them dynamically based other user interactions.

We support overriding various settings. These settings are optional and can be used to customize the conversation experience.

The following settings are available:

```ts
const conversation = useConversation({
  overrides: {
    agent: {
      prompt: {
        prompt: 'My custom prompt',
      },
      firstMessage: 'My custom first message',
      language: 'en',
    },
    tts: {
      voiceId: 'custom voice id',
    },
    conversation: {
      textOnly: true,
    },
  },
});
```

Use the prompt override to inject your own system prompt (for example, the Jadugar persona) and the `voiceId` override to pin the exact ElevenLabs voice you want the session to use:

```ts
await conversation.startSession({
  agentId: process.env.NEXT_PUBLIC_ELEVENLABS_AGENT_ID!,
  overrides: {
    agent: {
      prompt: { prompt: 'You are Jadugar, the Astra astrology companion...' },
    },
    tts: {
      voiceId: process.env.NEXT_PUBLIC_ELEVENLABS_VOICE_ID!,
    },
  },
  connectionType: 'webrtc',
  userId: 'user-123',
});
```

##### Text only

If your agent is configured to run in text-only mode, i.e. it does not send or receive audio messages, you can use this flag to use a lighter version of the conversation. In that case, the user will not be asked for microphone permissions and no audio context will be created.

```ts
const conversation = useConversation({
  textOnly: true,
});
```

##### Controlled State

You can control certain aspects of the conversation state directly through the hook options:

```ts
const [micMuted, setMicMuted] = useState(false);
const [volume, setVolume] = useState(0.8);

const conversation = useConversation({
  micMuted,
  volume,
  // ... other options
});

// Update controlled state
setMicMuted(true); // This will automatically mute the microphone
setVolume(0.5); // This will automatically adjust the volume
```

##### Data residency

You can specify which ElevenLabs server region to connect to. For more information see the [data residency guide](/docs/product-guides/administration/data-residency).

```ts
const conversation = useConversation({
  serverLocation: 'eu-residency', // or "us", "in-residency", "global"
});
```

#### Methods

##### startSession

The `startConversation` method kicks off the WebSocket or WebRTC connection and starts using the microphone to communicate with the ElevenLabs Agents agent. The method accepts an options object, with the `signedUrl`, `conversationToken` or `agentId` option being required.

The Agent ID can be acquired through [ElevenLabs UI](https://elevenlabs.io/app/agents).

We also recommended passing in your own end user IDs to map conversations to your users.

```js
const conversation = useConversation();

// For public agents, pass in the agent ID and the connection type
const conversationId = await conversation.startSession({
  agentId: '<your-agent-id>',
  connectionType: 'webrtc', // either "webrtc" or "websocket"
  userId: '<your-end-user-id>', // optional field
});
```

For public agents (i.e. agents that don't have authentication enabled), only the `agentId` is required.

In case the conversation requires authorization, use the REST API to generate signed links for a WebSocket connection or a conversation token for a WebRTC connection.

`startSession` returns a promise resolving a `conversationId`. The value is a globally unique conversation ID you can use to identify separate conversations.

<Tabs>
  <Tab title="WebSocket connection">
    ```js maxLines=0
    // Node.js server

    app.get("/signed-url", yourAuthMiddleware, async (req, res) => {
      const response = await fetch(
        `https://api.elevenlabs.io/v1/convai/conversation/get-signed-url?agent_id=${process.env.AGENT_ID}`,
        {
          headers: {
            // Requesting a signed url requires your ElevenLabs API key
            // Do NOT expose your API key to the client!
            "xi-api-key": process.env.ELEVENLABS_API_KEY,
          },
        }
      );

      if (!response.ok) {
        return res.status(500).send("Failed to get signed URL");
      }

      const body = await response.json();
      res.send(body.signed_url);
    });
    ```

    ```js
    // Client

    const response = await fetch("/signed-url", yourAuthHeaders);
    const signedUrl = await response.text();

    const conversation = await Conversation.startSession({
      signedUrl,
      connectionType: "websocket",
    });
    ```

  </Tab>
  <Tab title="WebRTC connection">
    ```js maxLines=0
    // Node.js server

    app.get("/conversation-token", yourAuthMiddleware, async (req, res) => {
      const response = await fetch(
        `https://api.elevenlabs.io/v1/convai/conversation/token?agent_id=${process.env.AGENT_ID}`,
        {
          headers: {
            // Requesting a conversation token requires your ElevenLabs API key
            // Do NOT expose your API key to the client!
            "xi-api-key": process.env.ELEVENLABS_API_KEY,
          }
        }
      );

      if (!response.ok) {
        return res.status(500).send("Failed to get conversation token");
      }

      const body = await response.json();
      res.send(body.token);
    );
    ```

    ```js
    // Client

    const response = await fetch("/conversation-token", yourAuthHeaders);
    const conversationToken = await response.text();

    const conversation = await Conversation.startSession({
      conversationToken,
      connectionType: "webrtc",
    });
    ```

  </Tab>
</Tabs>

##### endSession

A method to manually end the conversation. The method will disconnect and end the conversation.

```js
await conversation.endSession();
```

##### setVolume

Sets the output volume of the conversation. Accepts an object with a `volume` field between 0 and 1.

```js
await conversation.setVolume({ volume: 0.5 });
```

##### status

A React state containing the current status of the conversation.

```js
const { status } = useConversation();
console.log(status); // "connected" or "disconnected"
```

##### isSpeaking

A React state containing information on whether the agent is currently speaking. This is useful for indicating agent status in your UI.

```js
const { isSpeaking } = useConversation();
console.log(isSpeaking); // boolean
```

##### sendUserMessage

Sends a text message to the agent.

Can be used to let the user type in the message instead of using the microphone. Unlike `sendContextualUpdate`, this will be treated as a user message and will prompt the agent to take its turn in the conversation.

```js
const { sendUserMessage, sendUserActivity } = useConversation();
const [value, setValue] = useState("");

return (
  <>
    <input
      value={value}
      onChange={e => {
        setValue(e.target.value);
        sendUserActivity();
      }}
    />
    <button
      onClick={() => {
        sendUserMessage(value);
        setValue("");
      }}
    >
      SEND
    </button>
  </>
);
```

##### sendContextualUpdate

Sends contextual information to the agent that won't trigger a response.

```js
const { sendContextualUpdate } = useConversation();

sendContextualUpdate(
  "User navigated to another page. Consider it for next response, but don't react to this contextual update."
);
```

##### sendFeedback

Provide feedback on the conversation quality. This helps improve the agent's performance.

```js
const { sendFeedback } = useConversation();

sendFeedback(true); // positive feedback
sendFeedback(false); // negative feedback
```

##### sendUserActivity

Notifies the agent about user activity to prevent interruptions. Useful for when the user is actively using the app and the agent should pause speaking, i.e. when the user is typing in a chat.

The agent will pause speaking for ~2 seconds after receiving this signal.

```js
const { sendUserActivity } = useConversation();

// Call this when user is typing to prevent interruption
sendUserActivity();
```

##### canSendFeedback

A React state indicating whether feedback can be submitted for the current conversation.

```js
const { canSendFeedback } = useConversation();

// Use this to conditionally show feedback UI
{
  canSendFeedback && (
    <FeedbackButtons
      onLike={() => conversation.sendFeedback(true)}
      onDislike={() => conversation.sendFeedback(false)}
    />
  );
}
```

##### changeInputDevice

Switch the audio input device during an active voice conversation. This method is only available for voice conversations.

```js
// Change to a specific input device
await conversation.changeInputDevice({
  sampleRate: 16000,
  format: 'pcm',
  preferHeadphonesForIosDevices: true,
  inputDeviceId: 'your-device-id', // Optional: specific device ID
});
```

##### changeOutputDevice

Switch the audio output device during an active voice conversation. This method is only available for voice conversations.

```js
// Change to a specific output device
await conversation.changeOutputDevice({
  sampleRate: 16000,
  format: 'pcm',
  outputDeviceId: 'your-device-id', // Optional: specific device ID
});
```

<Note>
  Device switching only works for voice conversations. If no specific `deviceId` is provided, the
  browser will use its default device selection. You can enumerate available devices using the
  [MediaDevices.enumerateDevices()](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/enumerateDevices)
  API.
</Note>

##### getId

Returns the current conversation ID.

```js
const { getId } = useConversation();
const conversationId = getId();
console.log(conversationId); // e.g., "conv_abc123"
```

##### getInputVolume / getOutputVolume

Methods that return the current input/output volume levels (0-1 scale).

```js
const { getInputVolume, getOutputVolume } = useConversation();
const inputLevel = getInputVolume();
const outputLevel = getOutputVolume();
```

##### getInputByteFrequencyData / getOutputByteFrequencyData

Methods that return `Uint8Array`s containing the current input/output frequency data. See [AnalyserNode.getByteFrequencyData](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData) for more information.

```js
const { getInputByteFrequencyData, getOutputByteFrequencyData } = useConversation();
const inputFrequencyData = getInputByteFrequencyData();
const outputFrequencyData = getOutputByteFrequencyData();
```

<Note>
  These methods are only available for voice conversations. In WebRTC mode the audio is hardcoded to
  use `pcm_48000`, meaning any visualization using the returned data might show different patterns
  to WebSocket connections.
</Note>

##### sendMCPToolApprovalResult

Sends approval result for MCP (Model Context Protocol) tool calls.

```js
const { sendMCPToolApprovalResult } = useConversation();

// Approve a tool call
sendMCPToolApprovalResult('tool_call_id_123', true);

// Reject a tool call
sendMCPToolApprovalResult('tool_call_id_123', false);
```

##### MCP memory tooling

When your agent is connected to the Memory Store MCP server, surface approvals and connection status through the dedicated callbacks:

- Listen to `onMCPToolCall` to auto-approve trusted invocations (for example by checking whether a per-user token is present).
- Monitor `onMCPConnectionStatus` to keep the UI in sync with backend connectivity.
- Provide `dynamicVariables` such as `memory_store_token` and `julep_session_id` when starting the session so the ElevenLabs agent can authorize calls without prompting the end user.

Putting the pieces together:

```ts
const conversation = useConversation({
  onMCPToolCall: (call) => {
    if (call.service_id === 'memory-store' && call.state === 'awaiting_approval') {
      const token = window.sessionMemoryToken;
      sendMCPToolApprovalResult(call.tool_call_id, Boolean(token));
    }
  },
  onMCPConnectionStatus: ({ integrations }) => {
    console.log('MCP integrations', integrations);
  },
});

await conversation.startSession({
  signedUrl,
  connectionType: 'websocket',
  dynamicVariables: {
    memory_store_token: memoryStoreToken,
    julep_session_id: julepSessionId,
    user_name: userDisplayName,
  },
});
```

This mirrors the approach used in the Astra voice session: tokens are resolved server-side, passed in as dynamic variables, and the client quietly approves requests so the agent can write to Memory Store without interrupting the conversation.

## Agent configuration management

Per-session overrides let you experiment with prompts, voices, and MCP connectivity without mutating the underlying ElevenLabs agent. When you need a durable change—such as updating the system prompt Jadugar runs with, registering additional MCP servers, or attaching knowledge-base docs—call the Agents REST API directly.

### Update agent

PATCH https://api.elevenlabs.io/v1/convai/agents/{agent_id}  
Content-Type: application/json

Patches an Agent settings.

Reference: https://elevenlabs.io/docs/agents-platform/api-reference/agents/update

Common updates Jadugar needs:
- Persist your system prompt by patching `conversation_config.agent.prompt.prompt`.
- Register ElevenLabs with Julep memory by appending `native_mcp_server_ids` or `mcp_server_ids`.
- Lock a voice globally with `conversation_config.tts.voice_id` (plus `similarity_boost` or `stability` as needed).
- Attach knowledge base docs via the `knowledge_base` array or tune `rag` settings.
- Adjust workflow overrides when specific branches require different tools or prompts.

#### OpenAPI Specification

<details>
<summary>Expand for request shape</summary>

```yaml
openapi: 3.1.1
info:
  title: Update agent
  version: endpoint_conversationalAi/agents.update
paths:
  /v1/convai/agents/{agent_id}:
    patch:
      summary: Update agent
      description: Patches an Agent settings
      parameters:
        - name: agent_id
          in: path
          required: true
        - name: xi-api-key
          in: header
          required: true
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Body_Patches_an_Agent_settings_v1_convai_agents__agent_id__patch'
```

Refer to the full specification at [ElevenLabs Agents API reference](https://elevenlabs.io/docs/agents-platform/api-reference/agents/update) for every field (ASR, TTS, MCP server IDs, Knowledge Base locators, workflows, etc.).

</details>

#### SDK Code Examples

```go
package main

import (
    "fmt"
    "strings"
    "net/http"
    "io"
)

func main() {
    url := "https://api.elevenlabs.io/v1/convai/agents/agent_id"
    payload := strings.NewReader("{}")

    req, _ := http.NewRequest("PATCH", url, payload)
    req.Header.Add("xi-api-key", "xi-api-key")
    req.Header.Add("Content-Type", "application/json")

    res, _ := http.DefaultClient.Do(req)
    defer res.Body.Close()

    body, _ := io.ReadAll(res.Body)
    fmt.Println(res)
    fmt.Println(string(body))
}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.elevenlabs.io/v1/convai/agents/agent_id")
http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Patch.new(url)
request["xi-api-key"] = 'xi-api-key'
request["Content-Type"] = 'application/json'
request.body = "{}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse<String> response = Unirest.patch("https://api.elevenlabs.io/v1/convai/agents/agent_id")
  .header("xi-api-key", "xi-api-key")
  .header("Content-Type", "application/json")
  .body("{}")
  .asString();
```

```php
<?php

$client = new \GuzzleHttp\Client();

$response = $client->request('PATCH', 'https://api.elevenlabs.io/v1/convai/agents/agent_id', [
  'body' => '{}',
  'headers' => [
    'Content-Type' => 'application/json',
    'xi-api-key' => 'xi-api-key',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.elevenlabs.io/v1/convai/agents/agent_id");
var request = new RestRequest(Method.PATCH);
request.AddHeader("xi-api-key", "xi-api-key");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "xi-api-key": "xi-api-key",
  "Content-Type": "application/json"
]
let parameters = [] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(
  url: NSURL(string: "https://api.elevenlabs.io/v1/convai/agents/agent_id")! as URL,
  cachePolicy: .useProtocolCachePolicy,
  timeoutInterval: 10.0
)
request.httpMethod = "PATCH"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest) { data, response, error in
  if let error = error {
    print(error)
  } else if let httpResponse = response as? HTTPURLResponse {
    print(httpResponse)
  }
}

dataTask.resume()
```

```typescript
import { ElevenLabsClient } from "@elevenlabs/elevenlabs-js";

async function main() {
  const client = new ElevenLabsClient({
    environment: "https://api.elevenlabs.io",
  });
  await client.conversationalAi.agents.update("agent_id", {});
}
main();
```

```python
from elevenlabs import ElevenLabs

client = ElevenLabs(
    base_url="https://api.elevenlabs.io"
)

client.conversational_ai.agents.update(
    agent_id="agent_id"
)
```

### Create MCP server

Use this endpoint to register a new MCP server configuration (for example, Memory Store) before wiring it into your agent via `native_mcp_server_ids`.

POST https://api.elevenlabs.io/v1/convai/mcp-servers  
Content-Type: application/json

Reference: https://elevenlabs.io/docs/agents-platform/api-reference/mcp/create

Key fields:
- Provide `config.url` and pick the correct `config.transport` (`SSE` or `STREAMABLE_HTTP`).
- Embed auth with `config.secret_token` or structured `config.request_headers` (supports Julep secret locators).
- Choose `config.approval_policy` (or per-tool hashes) to match the approval UX you want in the client.

#### OpenAPI Specification

<details>
<summary>Expand for request shape</summary>

```yaml
openapi: 3.1.1
info:
  title: Create Mcp Server
  version: endpoint_conversationalAi/mcpServers.create
paths:
  /v1/convai/mcp-servers:
    post:
      summary: Create Mcp Server
      description: Create a new MCP server configuration in the workspace.
      parameters:
        - name: xi-api-key
          in: header
          required: true
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/MCPServerRequestModel'
```

See the reference for the complete schema (approval hashes, dependent agent metadata, transports, secrets).

</details>

#### SDK Code Examples

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.elevenlabs.io/v1/convai/mcp-servers"

	payload := strings.NewReader("{\n  \"config\": {\n    \"url\": \"string\",\n    \"name\": \"string\"\n  }\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("xi-api-key", "xi-api-key")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.elevenlabs.io/v1/convai/mcp-servers")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["xi-api-key"] = 'xi-api-key'
request["Content-Type"] = 'application/json'
request.body = "{\n  \"config\": {\n    \"url\": \"string\",\n    \"name\": \"string\"\n  }\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse<String> response = Unirest.post("https://api.elevenlabs.io/v1/convai/mcp-servers")
  .header("xi-api-key", "xi-api-key")
  .header("Content-Type", "application/json")
  .body("{\n  \"config\": {\n    \"url\": \"string\",\n    \"name\": \"string\"\n  }\n}")
  .asString();
```

```php
<?php

$client = new \GuzzleHttp\Client();

$response = $client->request('POST', 'https://api.elevenlabs.io/v1/convai/mcp-servers', [
  'body' => '{
  "config": {
    "url": "string",
    "name": "string"
  }
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'xi-api-key' => 'xi-api-key',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.elevenlabs.io/v1/convai/mcp-servers");
var request = new RestRequest(Method.POST);
request.AddHeader("xi-api-key", "xi-api-key");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"config\": {\n    \"url\": \"string\",\n    \"name\": \"string\"\n  }\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "xi-api-key": "xi-api-key",
  "Content-Type": "application/json"
]
let parameters = ["config": [
    "url": "string",
    "name": "string"
  ]] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.elevenlabs.io/v1/convai/mcp-servers")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```typescript
import { ElevenLabsClient } from "@elevenlabs/elevenlabs-js";

async function main() {
    const client = new ElevenLabsClient({
        environment: "https://api.elevenlabs.io",
    });
    await client.conversationalAi.mcpServers.create({
        config: {
            url: "string",
            name: "string",
        },
    });
}
main();
```

```python
from elevenlabs import ElevenLabs

client = ElevenLabs(
    base_url="https://api.elevenlabs.io"
)

client.conversational_ai.mcp_servers.create(
    config={
        "url": "string",
        "name": "string"
    }
)
```
