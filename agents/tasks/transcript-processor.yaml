name: Update User Overview From Transcript
description: >
  Merge the latest conversation transcript with the previous MongoDB user_overview and
  persist fresh memories to Memory Store when available.
project: astra

input_schema:
  type: object
  required:
    - julep_user_id
    - conversation_id
    - transcript_text
  properties:
    julep_user_id:
      type: string
      description: The Julep user identifier (maps 1:1 with the MongoDB user record)
    conversation_id:
      type: string
      description: ElevenLabs conversation identifier
    transcript_text:
      type: string
      description: Full transcript text for the conversation (already fetched upstream)
    existing_overview:
      type: object
      description: Latest MongoDB user_overview document or null when not available
    memory_store_token:
      type: string
      description: Optional Memory Store MCP token to persist long-term memories

tools:
  - name: mcp_discover
    description: Discover available Memory Store MCP tools
    type: integration
    integration:
      provider: mcp
      method: list_tools
      setup:
        transport: http
        http_url: $ f"https://beta.memory.store/mcp/?token={_.memory_store_token}"

  - name: mcp_execute
    description: Execute Memory Store MCP tools
    type: integration
    integration:
      provider: mcp
      method: call_tool
      setup:
        transport: http
        http_url: $ f"https://beta.memory.store/mcp/?token={_.memory_store_token}"

main:
  # Prep context for the LLM prompt
  - evaluate:
      overview_context: $ _.existing_overview or {}
      transcript_context: $ _.transcript_text.strip()
      overview_json: $ json.dumps(_.existing_overview or {}, indent=2)

  # Generate structured updates from transcript + existing overview
  - prompt: |-
      Extract insights from this astrology conversation and update the user profile.

      PREVIOUS USER DATA:
      {steps[0].output.overview_json}

      NEW CONVERSATION:
      {steps[0].output.transcript_context}

      TASK: Analyze the conversation and return JSON with updates. Only include fields that changed or are new. Use null for unchanged values.

      REQUIRED JSON FORMAT:
      {{
        "overview_updates": {{
          "profile_summary": "One-line user description or null",
          "preferences": {{
            "communication_style": "casual|balanced|formal|null",
            "topics_of_interest": ["array of topics"],
            "hinglish_level": 0-100,
            "flirt_opt_in": true|false|null,
            "astrology_system": "vedic|western|both|null"
          }},
          "insights": [
            {{"type": "insight|pattern", "content": "text", "generated_at": "ISO timestamp"}}
          ]
        }},
        "conversation_summary": {{
          "summary": "2-3 sentence recap",
          "topics": ["keywords"],
          "key_insights": ["insights"],
          "questions_asked": ["questions"],
          "emotional_tone": "1-2 words"
        }},
        "memories": [
          {{"title": "short title", "content": "what to remember", "importance": "low|normal|high"}}
        ],
        "birth_details": {{
          "city": "City or null",
          "country": "Country or null",
          "place_text": "Full location or null"
        }}
      }}

      RULES:
      1. Return ONLY valid JSON, no explanations
      2. Extract birth details ONLY if explicitly mentioned
      3. Detect Hinglish usage level (0=none, 100=high)
      4. Limit memories to important facts, not chitchat
      5. Use null for unchanged/unknown fields

      birth_date = {_.birth_date}
      birth_time = {_.birth_time}
      birth_location = {_.birth_location}
      birth_timezone = {_.birth_timezone}
    unwrap: true

  # Parse LLM output
  - evaluate:
      parsed: $ json.loads(steps[1].output)

  # Normalize optional sections
  - evaluate:
      overview_updates: $ steps[2].output.get("overview_updates", {})
      conversation_summary: $ steps[2].output.get("conversation_summary", {})
      memories: $ [m for m in steps[2].output.get("memories", []) if m.get("content")]
      birth_details: $ steps[2].output.get("birth_details", {})

  # Persist memories to Memory Store when a token is available
  - if: $ _.memory_store_token and len(steps[3].output.memories) > 0
    then:
      - tool: mcp_discover
        arguments: {}
      - evaluate:
          available_tools: $ steps[4].output.get("tools", []) if isinstance(steps[4].output, dict) else []
          store_tool: $ next((tool["name"] for tool in _.available_tools if tool["name"] in ["store", "store_memory", "remember", "write_memory"]), None)
      - if: $ _.store_tool is not None
        then:
          - tool: mcp_execute
            arguments:
              tool_name: $ _.store_tool
              arguments:
                entries: $ steps[3].output.memories
        else:
          - continue: null

  # Return payload for MongoDB + follow-up orchestration
  - return:
      julep_user_id: $ _.julep_user_id
      conversation_id: $ _.conversation_id
      overview_updates: $ steps[3].output.overview_updates
      conversation_summary: $ steps[3].output.conversation_summary
      memories: $ steps[3].output.memories
      birth_details: $ steps[3].output.birth_details
