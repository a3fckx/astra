name: Update User Overview From Transcript
description: >
  Merge the latest conversation transcript with the previous MongoDB user_overview and
  persist fresh memories to Memory Store when available.

input_schema:
  type: object
  required:
    - julep_user_id
    - conversation_id
    - transcript_text
  properties:
    julep_user_id:
      type: string
      description: The Julep user identifier (maps 1:1 with the MongoDB user record)
    conversation_id:
      type: string
      description: ElevenLabs conversation identifier
    transcript_text:
      type: string
      description: Full transcript text for the conversation (already fetched upstream)
    existing_overview:
      type: object
      description: Latest MongoDB user_overview document or null when not available
    memory_store_token:
      type: string
      description: Optional Memory Store MCP token to persist long-term memories (not yet implemented)

main:
  # Prep context for the LLM prompt
  - evaluate:
      overview_context: $ _.existing_overview or {}
      transcript_context: $ _.transcript_text.strip()
      overview_json: $ json.dumps(_.existing_overview or {}, indent=2)

  # Generate structured updates from transcript + existing overview
  - prompt: |-
      Extract insights from this astrology conversation and update the user profile.

      PREVIOUS USER DATA:
      {steps[0].output.overview_json}

      NEW CONVERSATION:
      {steps[0].output.transcript_context}

      TASK: Analyze the conversation and return JSON with updates. Only include fields that changed or are new. Use null for unchanged values. Capture notable "incidents" (creative sparks, emotional shifts, pivotal moments) that the agent can reference mysteriously later.

      ANCHOR: birth-data-extraction-rules
      CRITICAL BIRTH DATA RULES:
      - Birth DATE: NEVER extract (always exists from Google OAuth, ignore if mentioned)
      - Birth TIME: Extract ONLY if user explicitly mentions time of birth
      - Birth PLACE: Extract ONLY if user explicitly mentions birth location (city/country)
      - Format time naturally: "7am", "morning", "around 3pm" → Store as-is
      - Format place naturally: "Jhajjar, Haryana" or "New York" → Extract city and country

      ANCHOR: incident-map-rules
      INCIDENT MAP GUIDELINES:
      - Capture emotionally significant moments, creative sparks, pivotal realizations
      - Description should be 1-2 sentences, include temporal context naturally if relevant
      - Example: "Last week, user mentioned sudden inspiration for astro companion project"
      - Tags should be concise: ["creativity", "technology", "innovation"]
      - NO separate timestamp field - temporal info goes in description

      REQUIRED JSON STRUCTURE:
      Return a JSON object with these top-level keys:
      - overview_updates: object with profile_summary (string or null), preferences (object), insights (array)
      - conversation_summary: object with summary, topics array, key_insights array, questions_asked array, emotional_tone
      - memories: array of objects with title, content, importance (low/normal/high)
      - incident_map: array of objects with optional title, description (1-2 sentences), tags array
      - birth_details: object with city, country, place_text (all optional, only if explicitly mentioned)
      
      PREFERENCES OBJECT STRUCTURE:
      - communication_style: one of casual, balanced, formal, or null
      - topics_of_interest: array of topic strings
      - hinglish_level: number from 0 to 100
      - flirt_opt_in: boolean or null
      - astrology_system: one of vedic, western, both, or null

      CRITICAL OUTPUT REQUIREMENTS:
      - Return ONLY valid JSON - no explanations, no markdown, no code blocks
      - Start with opening brace, end with closing brace
      - Do NOT wrap in ```json or any other formatting
      
      EXTRACTION RULES:
      1. NEVER extract birth DATE (always exists from OAuth)
      2. Extract birth TIME and PLACE only if explicitly mentioned in conversation
      3. Detect Hinglish usage level (0=none, 100=high)
      4. Limit memories to important facts, not chitchat
      5. Use null for unchanged/unknown fields
      6. Incident descriptions should read naturally with temporal context embedded
    unwrap: true

  # Parse LLM output (strip markdown code blocks if present)
  - evaluate:
      cleaned_output: $ steps[1].output.strip().removeprefix('```json').removeprefix('```').removesuffix('```').strip()
  
  - evaluate:
      parsed: $ json.loads(steps[2].output.cleaned_output)

  # Normalize optional sections
  - evaluate:
      overview_updates: $ steps[3].output.parsed.get("overview_updates", {})
      conversation_summary: $ steps[3].output.parsed.get("conversation_summary", {})
      memories: $ [m for m in steps[3].output.parsed.get("memories", []) if m.get("content")]
      birth_details: $ steps[3].output.parsed.get("birth_details", {})
      incident_map: $ steps[3].output.parsed.get("incident_map", [])

  # ANCHOR:memory-store-integration
  # Note: MCP integration will be added in future iteration
  # For now, memories are stored in MongoDB user_overview

  # Generate next first_message for this user
  - prompt: |-
      Based on this conversation, craft a CATCHY, PERSONALIZED first message for the user's NEXT conversation.

      PREVIOUS OVERVIEW:
      {steps[0].output.overview_json}

      THIS CONVERSATION:
      {steps[0].output.transcript_context}

      TASK: If this conversation was meaningful and you learned something about the user, create a great opening line. If it was just small talk or very brief, return "SKIP".

      REQUIREMENTS (if creating message):
      - 1-2 sentences max  
      - Reference something specific from chart, conversation, or personality
      - Warm, engaging, makes them feel seen
      - Voice-optimized (no emojis)
      - Natural and conversational
      - MUST include [USERNAME] placeholder somewhere in the message for personalization

      Return ONLY:
      - The first message text with [USERNAME] placeholder included, OR
      - "SKIP" if conversation wasn't substantial enough
    unwrap: true

  # Process the generated first message
  - evaluate:
      generated_first_message: $ steps[5].output.strip() if steps[5].output else None
      should_update_first_message: $ steps[5].output and steps[5].output.strip() != "SKIP" and len(steps[5].output.strip()) > 10

  # ANCHOR:task-return-payload
  # Return payload for MongoDB + follow-up orchestration
  # All extracted data gets synced to MongoDB user_overview by transcript-processor.ts
  - return:
      julep_user_id: $ steps[0].input.julep_user_id
      conversation_id: $ steps[0].input.conversation_id
      overview_updates: $ steps[4].output.overview_updates
      conversation_summary: $ steps[4].output.conversation_summary
      memories: $ steps[4].output.memories
      birth_details: $ steps[4].output.birth_details
      incident_map: $ steps[4].output.incident_map
      first_message: $ steps[6].output.generated_first_message if steps[6].output.should_update_first_message else None
